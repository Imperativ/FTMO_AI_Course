# ğŸ“˜ Kapitel 3 â€“ ChatGPT-Integration vertiefen und Datenanalyse vorbereiten

**Lernziel:**
Nach dieser Lektion kannst du das aktuelle LLM-Modell gezielt als _Analystenkomponente_ in deinen Workflows einsetzen.
Du lernst, wie du strukturierte Marktdaten an die API Ã¼bergibst, eine saubere Antwort erhÃ¤ltst und daraus verwertbare Informationen machst.

---

## ğŸ§© Abschnitt 1 â€“ Warum KI im Trading nur so gut ist wie die Daten

Sprachmodelle kÃ¶nnen keine Kurse â€sehenâ€œ, sie verstehen nur, was du ihnen beschreibst.
Damit sie also eine Marktsituation bewerten kÃ¶nnen, musst du sie **in Text oder JSON-Form** Ã¼bersetzen.

Wenn du einem Modell bloÃŸ sagst:

> â€Analysiere BTC/USD.â€œ

dann weiÃŸ es nichts auÃŸer diesem Satz.
Wenn du hingegen sendest:

```json
{
  "symbol": "BTCUSD",
  "interval": "1h",
  "data": [
    {
      "time": "2025-11-03T10:00Z",
      "open": 68740,
      "high": 68810,
      "low": 68690,
      "close": 68795,
      "volume": 2410
    },
    {
      "time": "2025-11-03T11:00Z",
      "open": 68795,
      "high": 68950,
      "low": 68780,
      "close": 68920,
      "volume": 2554
    }
  ]
}
```

â€“ dann hat es einen Kontext, um z.â€¯B. Trend, Momentum oder VolatilitÃ¤t zu beurteilen.

---

## âš™ï¸ Abschnitt 2 â€“ Der GPT-Node in n8n

n8n stellt keinen eigenen GPT-Node â€out of the boxâ€œ bereit, aber du kannst dieselbe Funktion auf zwei Arten erreichen:

**Varianteâ€¯1:** Offizieller â€OpenAI Nodeâ€œ (falls installiert)
**Varianteâ€¯2:** â€HTTP Requestâ€œ Node â€“ universell und flexibel.

Wir nutzen hier Varianteâ€¯2, da sie immer verfÃ¼gbar ist.

1. FÃ¼ge in deinen Workflow einen **HTTP Request Node** ein.
2. Methode: `POST`
3. URL:
   ```
   https://api.openai.com/v1/chat/completions
   ```
4. Header:
   ```
   Authorization: Bearer {{$env.OPENAI_API_KEY}}
   Content-Type: application/json
   ```
5. Body:
   ```json
   {
     "model": "aktuelles LLM Modell",
     "messages": [
       { "role": "system", "content": "Du bist ein Finanzdaten-Analyst." },
       { "role": "user", "content": "Analysiere folgende Kursdaten: {{$json}}" }
     ]
   }
   ```

Ergebnis: Der Output des Nodes enthÃ¤lt ein JSON-Feld `choices[0].message.content` mit der Analyse.

---

## ğŸ§  Abschnitt 3 â€“ Gute Prompts fÃ¼r strukturierte Ausgaben

Um aus LLMs mehr als nur Prosa zu bekommen, trainierst du sie Ã¼ber **Prompt-Struktur**.

Schlechtes Beispiel:

> â€Was hÃ¤ltst du von diesen Kursen?â€œ

Besser:

> â€Analysiere die Kursdaten. Gib dein Ergebnis als JSON mit den Feldern
> `trend`, `momentum`, `risk_level`, `recommendation` zurÃ¼ck.â€œ

Ergebnis:

```json
{
  "trend": "bullish",
  "momentum": 0.72,
  "risk_level": "low",
  "recommendation": "long"
}
```

Dieser Output kann im nÃ¤chsten Node direkt weiterverarbeitet werden, z.â€¯B. in einem Telegram-Alert oder einem eigenen Dashboard.

---

## ğŸ’¡ Abschnitt 4 â€“ Versionen und Tests

Bevor du produktiv arbeitest:

- PrÃ¼fe die n8n-Version (`n8n --version` oder MenÃ¼ â€Help â†’ Aboutâ€œ).
- Teste API-Keys mit minimalen Requests, um Rate-Limits zu kennen.
- Achte darauf, dass das Modell-Feld in deinem HTTP-Node dem tatsÃ¤chlich verfÃ¼gbaren Modell entspricht.

So bleibt dein Workflow zukunftssicher, falls Anbieter Endpunkte Ã¤ndern.

---

## âš ï¸ Abschnitt 5 â€“ Bewusster Umgang mit Risiken

Dieses System bewertet Marktdaten anhand von Sprachmodellen.
Es ist kein Garant fÃ¼r Profit, sondern ein Werkzeug, um **Hypothesen zu generieren**.
Du bleibst stets Entscheider.
Ein gutes Demokonto ist daher nicht nur Ãœbungsumgebung, sondern Sicherheitsgurt.

---

## ğŸ§© Abschnitt 6 â€“ Mini-Praxis: Dein erster Daten-Analyst

1. Baue in n8n folgenden Flow:
   - Nodeâ€¯1: **Webhook** (`POST`).
   - Nodeâ€¯2: **Function Node**, die Beispiel-Marktdaten erzeugt (wie oben gezeigt).
   - Nodeâ€¯3: **HTTP Request Node** (aktuelles LLM-Modell).
   - Nodeâ€¯4: **Telegram Node**, sendet `choices[0].message.content`.

2. Teste den Flow durch manuelles AuslÃ¶sen des Webhooks.

3. Beobachte die Antwort:
   - Liefert sie eine Struktur?
   - EnthÃ¤lt sie quantitative Aussagen (z.â€¯B. â€Momentumâ€¯0.8â€œ)?
   - Kannst du das JSON weiterverwenden?

---

## ğŸ§­ Abschnitt 7 â€“ Reflexion

- Wie klar war deine Kommunikation mit dem Modell?
- Was passiert, wenn du die Datenmenge vergrÃ¶ÃŸerst â€“ wird die Antwort diffuser?
- Wie wÃ¼rdest du den Prompt umformulieren, um prÃ¤zisere Analysen zu erhalten?

Schreibe drei Varianten deines Prompts auf und vergleiche die Outputs.
Das ist praktisches Prompt-Tuning â€“ die Grundlage fÃ¼r deinen spÃ¤teren Agenten.

---

## ğŸ§© Abschnitt 8 â€“ Hausaufgabe / Experiment

1. ErgÃ¤nze den Workflow um einen **Switch-Node**, der bei `"trend": "bullish"` automatisch eine andere Nachricht sendet als bei `"bearish"`.
2. Logge die Ausgabe zusÃ¤tzlich in einer Datei (`Write Binary File` Node).
3. Dokumentiere in deiner Kursmappe, wie du Prompt- und Node-Einstellungen verÃ¤ndert hast, um konsistente JSON-Outputs zu erhalten.

---

## ğŸš¨ Abschnitt 9 â€“ API & LLM Debugging

### OpenAI API gibt 401 zurÃ¼ck

**Problem:** "Invalid API key"
**Diagnose-Schritte:**

1. API-Key in n8n Environment prÃ¼fen: Settings â†’ Environment Variables
2. Test-Call mit curl:
   ```bash
   curl -H "Authorization: Bearer YOUR_KEY" \
        -H "Content-Type: application/json" \
        https://api.openai.com/v1/models
   ```
3. Rate-Limits checken: API-Response-Header `x-ratelimit-remaining`

### LLM gibt inkonsistente JSON zurÃ¼ck

**Problem:** Mal `{"trend":"bullish"}`, mal `Trend: bullish (Text)`
**Debugging-Prompt:**

```
Antworte ausschlieÃŸlich im folgenden JSON-Format, ohne zusÃ¤tzlichen Text:
{"trend": "bullish|bearish|neutral", "confidence": 0.XX}

Beispiel: {"trend": "bullish", "confidence": 0.73}
```

**Validation-Node hinzufÃ¼gen:**

```javascript
try {
  const response = JSON.parse($json.choices[0].message.content);
  return [response];
} catch (e) {
  return [{ error: "Invalid JSON", raw: $json.choices[0].message.content }];
}
```

### Timeout-Probleme

**Problem:** HTTP-Request hÃ¤ngt oder bricht ab
**LÃ¶sung:**

- HTTP-Node â†’ Options â†’ Timeout auf 30000ms setzen
- Retry-Logik: Options â†’ "Retry on Fail" aktivieren
- Fallback-Strategy mit Switch-Node fÃ¼r Error-Handling

### Error-Handling Template

FÃ¼r kritische API-Calls:

```javascript
try {
  const result = $json.choices[0].message.content;
  const parsed = JSON.parse(result);
  return [{ success: true, data: parsed }];
} catch (error) {
  return [
    {
      success: false,
      error: error.message,
      timestamp: new Date().toISOString(),
      input: $json,
    },
  ];
}
```

---

## âœ… Zusammenfassung

Nach Kapitel 3 kannst du:

- das aktuelle LLM-Modell gezielt Ã¼ber n8n ansprechen,
- strukturierte Daten einspeisen und verwertbare JSON-Analysen erzeugen,
- und die Ergebnisse logisch weiterverarbeiten.

Im nÃ¤chsten Kapitel geht es um den Schritt von **Datenanalyse zu Entscheidungslogik** â€“ wie dein Agent Risiko kalkuliert, PositionsgrÃ¶ÃŸen vorschlÃ¤gt und sich in grÃ¶ÃŸere Strategien einbettet.
